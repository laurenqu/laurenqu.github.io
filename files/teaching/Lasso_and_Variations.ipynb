{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECMA 31350: Lasso and Variations  \n",
    "### 3rd TA Discussion – Post-Lasso, Partial Penalization, and Double Lasso  \n",
    "**Date:** April 9, 2025  \n",
    "**TA:** Lauren Qu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sala-i-Martin (1997) \"I Just Ran Two Million Regressions\"\n",
    "\n",
    "### Background\n",
    "\n",
    "The goal of the study is to identify the determinants of long-run economic growth across countries.\n",
    "The dataset includes 62 candidate explanatory variables\n",
    "The author estimates all possible regressions using only 7 variables at a time\n",
    "\n",
    "###  Method & Assumption\n",
    "Brute-force approach, 2mil regressions, tracks which variables appear significantly more often than others\n",
    "\n",
    "###  Problem\n",
    "Computationally infeasible when the number of covariates p is large\n",
    "Subset selection becomes intractable as p grows (combinatorial explosion)\n",
    "\n",
    "### Motivation for Lasso\n",
    "Lasso offers a convex relaxation of the best subset selection problem\n",
    "It efficiently performs variable selection without searching all 2^p combinations\n",
    "Suitable when we believe only a few predictors are relevant (i.e., sparsity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Post-Lasso  \n",
    "2. Partial Penalization  \n",
    "3. Double Lasso (with orthogonalization logic)  \n",
    "4. Neyman Orthogonality  \n",
    "\n",
    "\n",
    "### 1. Post-Lasso\n",
    "\n",
    "#### Definition\n",
    "Lasso performs both variable selection and shrinkage. However, the shrinkage introduces bias, especially when the true signal is strong. Post-Lasso is designed to remove the shrinkage bias while retaining variable selection.\n",
    "\n",
    "#### Procedure\n",
    "1. Run Lasso on $Y \\sim X$ and obtain the set of selected variables:\n",
    "   $$\n",
    "   \\hat{J}_n = \\{j: \\hat{\\beta}^{Lasso}_j \\neq 0\\}\n",
    "   $$\n",
    "2. Run OLS on the selected covariates $X_j$, $j \\in \\hat{J}_n$\n",
    "\n",
    "#### Advantage\n",
    "- Unbiased estimation after variable selection\n",
    "- Works well when the true model is sparse and selection is accurate\n",
    "\n",
    "#### Reference\n",
    "Belloni and Chernozhukov (2009) show that Post-Lasso retains the Lasso rate of convergence and reduces bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Partial Penalization\n",
    "\n",
    "#### Motivation\n",
    "In many applications, some variables are theoretically important (e.g., treatment, price, fixed effects) and should **not be penalized** during Lasso regularization.\n",
    "\n",
    "#### Model\n",
    "Let $X = (D, W)$, where:\n",
    "- $D \\in \\mathbb{R}^d$: important variables (not penalized)\n",
    "- $W \\in \\mathbb{R}^p$: high-dimensional controls (penalized)\n",
    "\n",
    "The estimation problem becomes:\n",
    "$$\n",
    "\\min_{b_1, b_2} \\frac{1}{n} \\sum_{i=1}^n (Y_i - D_i'b_1 - W_i'b_2)^2 + \\lambda \\sum_{j=1}^p |b_{2j}|\n",
    "$$\n",
    "\n",
    "#### Implementation\n",
    "Partial out $D$ from both $Y$ and $W$ using the Frisch-Waugh-Lovell theorem, then perform Lasso on the residualized data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Double Lasso: Estimation of Treatment Effects with High-Dimensional Controls\n",
    "\n",
    "#### Goal\n",
    "Estimate the treatment effect $\\beta_1$ in:\n",
    "$$\n",
    "Y = D \\beta_1 + W'\\beta_2 + \\varepsilon,\\quad \\mathbb{E}[\\varepsilon|D, W] = 0\n",
    "$$\n",
    "when $W$ is high-dimensional and potentially correlated with both $D$ and $Y$.\n",
    "\n",
    "#### Problem\n",
    "Lasso may omit variables highly correlated with $D$, leading to omitted variable bias.\n",
    "\n",
    "#### Double Lasso Procedure\n",
    "1. Run Lasso of $D \\sim W$ → obtain $\\hat{\\gamma}$\n",
    "2. Run Lasso of $Y \\sim D + W$ → obtain $\\hat{\\beta}_2$\n",
    "3. Estimate $\\beta_1$ via orthogonal moment:\n",
    "$$\n",
    "\\hat{\\beta}_1 = \\frac{\\frac{1}{n} \\sum_{i=1}^n (Y_i - W_i'\\hat{\\beta}_2)(D_i - W_i'\\hat{\\gamma})}{\\frac{1}{n} \\sum_{i=1}^n D_i (D_i - W_i'\\hat{\\gamma})}\n",
    "$$\n",
    "\n",
    "#### Reference\n",
    "Belloni, Chernozhukov, and Hansen (2014), *Review of Economic Studies*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Belloni, Chernozhukov, and Hansen (2014) – *“Inference on Treatment Effects after Selection among High-Dimensional Controls” \n",
    "\n",
    "## Goal\n",
    "Estimate the **causal effect** of a treatment or policy variable (e.g., education, treatment assignment, price) **in the presence of many control variables**, where:\n",
    "- The number of controls $p$ may be large relative to $n$\n",
    "- The relevant control variables are assumed to be **sparse**\n",
    "\n",
    "## Model Setup\n",
    "\n",
    "Main model:\n",
    "$$\n",
    "Y = D \\beta_1 + W'\\beta_2 + \\varepsilon, \\quad \\mathbb{E}[\\varepsilon | D, W] = 0\n",
    "$$\n",
    "\n",
    "Auxiliary model (for selection bias correction):\n",
    "$$\n",
    "D = W'\\gamma + \\nu, \\quad \\mathbb{E}[\\nu | W] = 0\n",
    "$$\n",
    "\n",
    "- $D$: treatment or policy variable (e.g., education, treatment assignment)\n",
    "- $W$: high-dimensional control variables\n",
    "- Lasso is used to select relevant $W$'s from both models\n",
    "\n",
    "\n",
    "## Key Insight: Neyman Orthogonality\n",
    "\n",
    "They construct a moment condition that is **robust to small errors in nuisance parameters** $\\beta_2, \\gamma$:\n",
    "$$\n",
    "\\psi(Y, D, W; \\beta_1) = (Y - D\\beta_1 - W'\\beta_2)(D - W'\\gamma)\n",
    "$$\n",
    "This condition satisfies:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\beta_2} \\mathbb{E}[\\psi] = 0, \\quad \\frac{\\partial}{\\partial \\gamma} \\mathbb{E}[\\psi] = 0\n",
    "$$\n",
    "\n",
    " This means the estimator for $\\beta_1$ remains consistent even if the selection of controls is imperfect!\n",
    "\n",
    "\n",
    "##  Estimation Steps: Double Lasso\n",
    "\n",
    "1. Run Lasso of $D$ on $W$ → get $\\hat{\\gamma}$\n",
    "2. Run Lasso of $Y$ on $D$ and $W$ → get $\\hat{\\beta}_2$\n",
    "3. Estimate $\\hat{\\beta}_1$ using the orthogonal moment:\n",
    "$$\n",
    "\\hat{\\beta}_1 = \\frac{ \\frac{1}{n} \\sum_{i=1}^n (Y_i - W_i'\\hat{\\beta}_2)(D_i - W_i'\\hat{\\gamma}) }{ \\frac{1}{n} \\sum_{i=1}^n D_i(D_i - W_i'\\hat{\\gamma}) }\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated treatment effect (Double Lasso): 1.4377\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Simulate data\n",
    "np.random.seed(42)\n",
    "n, p = 500, 100\n",
    "W = np.random.randn(n, p)\n",
    "true_gamma = np.zeros(p); true_gamma[:2] = [0.5, 0.3]\n",
    "D = W @ true_gamma + np.random.randn(n)\n",
    "true_beta2 = np.zeros(p); true_beta2[:1] = [0.7]\n",
    "Y = 1.5 * D + W @ true_beta2 + np.random.randn(n)\n",
    "\n",
    "# Standardize W\n",
    "scaler = StandardScaler()\n",
    "W_scaled = scaler.fit_transform(W)\n",
    "\n",
    "# Step 1: Lasso of D ~ W\n",
    "lasso_D = LassoCV(cv=5).fit(W_scaled, D)\n",
    "gamma_hat = lasso_D.coef_\n",
    "\n",
    "# Step 2: Lasso of Y ~ D + W\n",
    "X_full = np.column_stack((D, W_scaled))\n",
    "lasso_Y = LassoCV(cv=5).fit(X_full, Y)\n",
    "beta_full = lasso_Y.coef_\n",
    "beta1_tilde = beta_full[0]            # coefficient on D\n",
    "beta2_hat = beta_full[1:]            # coefficients on W\n",
    "\n",
    "# Step 3: Orthogonalized estimation of beta_1\n",
    "W_gamma = W_scaled @ gamma_hat\n",
    "W_beta2 = W_scaled @ beta2_hat\n",
    "\n",
    "numerator = np.mean((Y - W_beta2) * (D - W_gamma))\n",
    "denominator = np.mean(D * (D - W_gamma))\n",
    "beta1_hat = numerator / denominator\n",
    "\n",
    "print(f\"Estimated treatment effect (Double Lasso): {beta1_hat:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive OLS estimate on D: 1.4243\n"
     ]
    }
   ],
   "source": [
    "# Naive OLS (bad if p > n or omitted variable bias exists)\n",
    "X_ols = sm.add_constant(np.column_stack((D, W_scaled)))\n",
    "ols_model = sm.OLS(Y, X_ols).fit()\n",
    "print(f\"Naive OLS estimate on D: {ols_model.params[1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Post-Lasso, Partial Penalization, and Double Lasso\n",
    "\n",
    "| Feature / Method           | Post-Lasso                          | Partial Penalization                 | Double Lasso                                 |\n",
    "|---------------------------|-------------------------------------|--------------------------------------|----------------------------------------------|\n",
    "| **Goal**                  | Reduce bias after variable selection| Avoid penalizing key regressors      | Obtain valid causal inference with many controls |\n",
    "| **Penalized Variables**   | All initially penalized             | Only subset of covariates penalized  | Only controls $W$ penalized              |\n",
    "| **Bias**                  | Reduced (compared to Lasso)         | Reduced for key regressors           | Robust via orthogonalization                 |\n",
    "| **Consistency for β₁**    | No guarantee                        | No guarantee                         | Yes (under regularity + orthogonality)       |\n",
    "| **Interpretation**        | Improves estimation                 | Theory-driven modeling flexibility   | Supports valid inference                     |\n",
    "| **Assumption**            | Correct model selection             | Known key variables                  | Approx. sparsity + moment orthogonality      |\n",
    "| **Inference possible?**   | Risky (depends on selection accuracy)| Risky (unless known model)          | Yes (asymptotic normality holds)             |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Recommended Use-Cases\n",
    "\n",
    "### 1. **Post-Lasso**\n",
    "\n",
    "- **When to use**:\n",
    "  - You care about **prediction** or **point estimates**, but not inference\n",
    "  - You believe Lasso selects the right variables\n",
    "  - You want to reduce shrinkage bias\n",
    "  \n",
    "- **Example**:\n",
    "  - **Belloni & Chernozhukov (2009)**:\n",
    "    *“Least Squares After Model Selection in High-Dimensional Sparse Models”*, published in *Bernoulli*  \n",
    "    > Shows Post-Lasso often dominates Lasso in mean-squared error if selection is correct\n",
    "\n",
    "\n",
    "### 2. **Partial Penalization**\n",
    "\n",
    "- **When to use**:\n",
    "  - You **must include certain variables** due to theory (e.g., price, policy dummies, fixed effects)\n",
    "  - You want flexible model selection for nuisance controls\n",
    "  \n",
    "- **Example**:\n",
    "  - **DellaVigna & Gentzkow (2019)**:\n",
    "    *“Uniform Pricing in U.S. Retail Chains”*, *Quarterly Journal of Economics*  \n",
    "    > Important regressors like prices are **never penalized**, but other store-level or region-level controls are selected flexibly\n",
    "\n",
    "\n",
    "\n",
    "### 3. **Double Lasso**\n",
    "\n",
    "- **When to use**:\n",
    "  - You want to **estimate a causal effect** of a treatment with many potential controls\n",
    "  - You are concerned about **omitted variable bias**\n",
    "  - You need **valid standard errors** and confidence intervals\n",
    "  \n",
    "- **Examples**:\n",
    "\n",
    "  1. **Belloni, Chernozhukov, Hansen (2014)**  \n",
    "     *“Inference on Treatment Effects After Selection Among High-Dimensional Controls”*, *Review of Economic Studies*  \n",
    "     > Canonical paper introducing Double Lasso for estimating treatment effects\n",
    "\n",
    "  2. **Chernozhukov et al. (2015)**  \n",
    "     *“Valid Post-Selection and Post-Regularization Inference: An Elementary, General Approach”*, *Annual Review of Economics*  \n",
    "     > Introduces Neyman orthogonality in a general double/debiased ML framework\n",
    "\n",
    "  3. **Bartik et al. (2020)**  \n",
    "     *“Using Machine Learning to Estimate Heterogeneous Treatment Effects”*, *AER: Insights*  \n",
    "     > Combines Double Lasso and causal forests for robust treatment effect estimation\n",
    "\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "- Use **Post-Lasso** when your goal is **better prediction** and you trust Lasso’s variable selection\n",
    "- Use **Partial Penalization** when **economic theory mandates inclusion** of certain variables\n",
    "- Use **Double Lasso** when you aim for **valid causal inference** in high-dimensional settings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
